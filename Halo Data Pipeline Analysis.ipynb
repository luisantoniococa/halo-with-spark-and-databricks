{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halo Data Pipelines and Analysis\n",
    "- The goal of this notebook is to provide a basic analysis of some of the data created for Halo games data.\n",
    "- For this notebook, pyspark in databricks was utilized to read some of the data.\n",
    "- The database was obtained from a Data Engineering Bootcamp.\n",
    "## Tasks\n",
    "- Create a Spark job that analyzes the medal counts of each player (one day at a time)\n",
    "  - Create tables medals, \n",
    "    - medals\n",
    "      - (doesn't need to be date partitioned since they don't change)\n",
    "    - matches \n",
    "      - (should be date partitioned)\n",
    "    - medals_matches_players \n",
    "      - (should be date partitioned)\n",
    "  - Explicitly broadcast join medals to medals_matches_players (medals is a small table)\n",
    "  - Create a DDL for a date partitioned table `daily_player_medal_counts`\n",
    "    - This should have one row per player per medal per day\n",
    "    - Play around with the sorting of this table to find what compressions best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77af78a2-e963-4395-a9ad-579e1830ac94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ef1abf-20c4-491a-816e-ed9812dc2385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PlayerMedalCounts\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ce0892d-538f-4c4e-8831-74995192fc1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee098bb-4881-4f3d-8186-d7fb69e3d21d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'matches''medals''medals_matches_players'+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|         device_data|      false|\n",
      "| default|             devices|      false|\n",
      "| default|          event_data|      false|\n",
      "| default|              events|      false|\n",
      "| default|             matches|      false|\n",
      "| default|              medals|      false|\n",
      "| default|medals_matches_pl...|      false|\n",
      "| default|           user_data|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, broadcast, split, lit, count, sum\n",
    "\n",
    "\n",
    "\n",
    "# Set the autoBroadcastJoinThreshold configuration\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10000000000\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Postgres credentials\n",
    "driver = \"org.postgresql.Driver\"\n",
    "database_host = \"\"\n",
    "database_port = \"5432\" # update if you use a non-default port\n",
    "database_name = \"\" # eg. postgres\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "url = \"\"\n",
    "\n",
    "tables = ['matches','medals','medals_matches_players']\n",
    "\n",
    "for table in tables:\n",
    "    remote_table = spark.read\\\n",
    "        .format(\"jdbc\")\\\n",
    "        .option(\"driver\", driver)\\\n",
    "        .option(\"url\", url)\\\n",
    "        .option(\"dbtable\", table)\\\n",
    "        .option(\"user\", user)\\\n",
    "        .option(\"password\", password)\\\n",
    "        .load()\n",
    "    \n",
    "    # Save matches table as a parquet table\n",
    "    remote_table.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(table)\n",
    "    display(table)\n",
    "\n",
    "\n",
    "spark.sql(\"show tables\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8498feeb-7381-4c62-9fab-ffe92bf34b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import medals table\n",
    "table = \"medals\"\n",
    "\n",
    "medals = spark.table(table)\n",
    "\n",
    "# medals.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515c1b75-b8ef-4948-b444-e776da9855eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- mapid: string (nullable = true)\n",
      " |-- is_team_game: boolean (nullable = true)\n",
      " |-- playlist_id: string (nullable = true)\n",
      " |-- game_variant_id: string (nullable = true)\n",
      " |-- is_match_over: boolean (nullable = true)\n",
      " |-- completion_date: timestamp (nullable = true)\n",
      " |-- match_duration: string (nullable = true)\n",
      " |-- game_mode: string (nullable = true)\n",
      " |-- map_variant_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import matches table\n",
    "table = \"matches\"\n",
    "matches = spark.table(table)\n",
    "matches.printSchema()\n",
    "\n",
    "matches = matches.repartition(\"completion_date\")\n",
    "\n",
    "# matches.explain()\n",
    "# matches.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d3edb7-7c8c-41f1-a775-144dc75b900a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- player_gamertag: string (nullable = true)\n",
      " |-- medal_id: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+--------------------+---------------+----------+-----+\n",
      "|            match_id|player_gamertag|  medal_id|count|\n",
      "+--------------------+---------------+----------+-----+\n",
      "|009fdac5-e15c-47c...|       EcZachly|3261908037|    7|\n",
      "|009fdac5-e15c-47c...|       EcZachly| 824733727|    2|\n",
      "|009fdac5-e15c-47c...|       EcZachly|2078758684|    2|\n",
      "|009fdac5-e15c-47c...|       EcZachly|2782465081|    2|\n",
      "|9169d1a3-955c-4ea...|       EcZachly|3001183151|    1|\n",
      "+--------------------+---------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import medals_matches_players table\n",
    "table = \"medals_matches_players\"\n",
    "medals_matches_players = spark.table(table)\n",
    "\n",
    "\n",
    "medals_matches_players.printSchema()\n",
    "medals_matches_players.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "929dcf87-07b9-4908-ae6d-e8c4054b13c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join matches and medals_matches_players on match_id column and then partition by matches.completion_date\n",
    "partitioned_medals_matches_players = medals_matches_players\\\n",
    "    .join(matches, \"match_id\")\\\n",
    "    .select(\n",
    "            [\"player_gamertag\",\n",
    "            \"match_id\",\n",
    "            \"medal_id\",\n",
    "            \"count\",\n",
    "            \"completion_date\"]\n",
    "            )\\\n",
    "    .repartition(\"completion_date\")\n",
    "\n",
    "# Save new combined table table as a parquet table\n",
    "partitioned_medals_matches_players.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"partitioned_medals_matches_players\")\n",
    "partitioned_medals_matches_players = spark.table(\"partitioned_medals_matches_players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d2e955-5e1d-4519-aaf9-b79187eda522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------+-----+-------------------+\n",
      "|player_gamertag|            match_id|  medal_id|count|    completion_date|\n",
      "+---------------+--------------------+----------+-----+-------------------+\n",
      "| Panda Bearsack|01e5f0bd-8382-4b2...|2287626681|    1|2015-12-27 00:00:00|\n",
      "| Panda Bearsack|01e5f0bd-8382-4b2...|3261908037|    1|2015-12-27 00:00:00|\n",
      "|          CJ700|01e5f0bd-8382-4b2...|3261908037|    9|2015-12-27 00:00:00|\n",
      "|          CJ700|01e5f0bd-8382-4b2...|2078758684|    3|2015-12-27 00:00:00|\n",
      "|          CJ700|01e5f0bd-8382-4b2...|2430242797|    1|2015-12-27 00:00:00|\n",
      "+---------------+--------------------+----------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partitioned_medals_matches_players.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0cdd1f-8d89-4b58-b782-1be30817da53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_player_medal_counts = partitioned_medals_matches_players\\\n",
    "    .repartition(4)\\\n",
    "    .join(broadcast(medals), \"medal_id\")\\\n",
    "    .select(\n",
    "        partitioned_medals_matches_players[\"*\"],\n",
    "        split(partitioned_medals_matches_players[\"completion_date\"], \" \").getItem(0).alias(\"ds\")\n",
    "    )\n",
    "joined_player_medal_counts.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"joined_player_medal_counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47462f0d-2667-4559-9efe-c66dcae3b2af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- player_gamertag: string (nullable = true)\n",
      " |-- match_id: string (nullable = true)\n",
      " |-- medal_id: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- completion_date: timestamp (nullable = true)\n",
      " |-- ds: string (nullable = true)\n",
      "\n",
      "+---------------+--------------------+----------+-----+-------------------+----------+\n",
      "|player_gamertag|            match_id|  medal_id|count|    completion_date|        ds|\n",
      "+---------------+--------------------+----------+-----+-------------------+----------+\n",
      "|Killswitch V7II|4d163705-306f-4a0...|2287626681|    1|2015-12-23 00:00:00|2015-12-23|\n",
      "|blue devil 2121|7ed01b67-7916-4a0...|1351381581|    8|2015-12-02 00:00:00|2015-12-02|\n",
      "|    Tanner Haze|0cea6755-a0a7-40b...| 824733727|    1|2016-03-15 00:00:00|2016-03-15|\n",
      "|           BMMV|d44d2577-da35-413...|3261908037|    3|2016-04-12 00:00:00|2016-04-12|\n",
      "|    MUFFINSCRUB|d7da9c6e-339a-450...|2287626681|    1|2016-01-23 00:00:00|2016-01-23|\n",
      "+---------------+--------------------+----------+-----+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_player_medal_counts.printSchema()\n",
    "joined_player_medal_counts.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a DDL for a date partitioned table `daily_player_medal_counts`\n",
    "\n",
    "- This should have one row per player per medal per day\n",
    "- Play around with the sorting of this table to find what compressions best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd92991-5f65-4b17-b335-8682181d2bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tableName = \"daily_player_medal_counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c8d1434-bc58-43f4-b45f-35aa7b3013d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+------------+----------+\n",
      "|player_gamertag|  medal_id|medal_counts|        ds|\n",
      "+---------------+----------+------------+----------+\n",
      "|   A 2tha nimal|2078758684|           1|2016-01-14|\n",
      "|   A 2tha nimal|2287626681|           1|2016-01-14|\n",
      "|   A 2tha nimal|3261908037|           4|2016-01-14|\n",
      "|   A 2tha nimal|3491849182|           1|2016-01-14|\n",
      "|   A 2tha nimal| 824733727|           1|2016-01-14|\n",
      "+---------------+----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DDL for daily_player_medal_counts\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {tableName}\")\n",
    "\n",
    "daily_player_medal_counts_ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {tableName} (\n",
    "       player_gamertag STRING,    \n",
    "       medal_id STRING,\n",
    "       medal_counts INT\n",
    "    )\n",
    "    USING PARQUET\n",
    "    PARTITIONED BY (ds STRING)\n",
    "    LOCATION '/{tableName}'\n",
    "\"\"\"\n",
    "spark.sql(daily_player_medal_counts_ddl)\n",
    "\n",
    "joined_player_medal_counts = spark.table(\"joined_player_medal_counts\")\n",
    "# joined_player_medal_counts.show(5)\n",
    "\n",
    "daily_player_medal_counts = joined_player_medal_counts\\\n",
    "            .groupBy(\n",
    "               joined_player_medal_counts.player_gamertag,\n",
    "               joined_player_medal_counts.medal_id,\n",
    "               joined_player_medal_counts.ds\n",
    "            )\\\n",
    "            .agg(\n",
    "                sum(joined_player_medal_counts[\"count\"]).alias(\"medal_counts\") # use medals_matches_players.count ?\n",
    "            )\\\n",
    "            .select(\n",
    "                [\"player_gamertag\",\n",
    "                \"medal_id\",\n",
    "                \"medal_counts\",\n",
    "                \"ds\"]\n",
    "             )\\\n",
    "            .repartition(\"ds\")\\\n",
    "            .sortWithinPartitions(\"player_gamertag\",\"medal_id\")\n",
    "\n",
    "# Write the daily_player_medal_counts DataFrame, repartitioning to 1 file per partition\n",
    "daily_player_medal_counts.repartition(1).write.format(\"parquet\").mode(\"overwrite\").insertInto(tableName)\n",
    "\n",
    "daily_player_medal_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd48ddb2-2416-45aa-abc9-bf6d5d7940fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [Row(col_name='player_gamertag', data_type='string', comment=None),\n",
      " Row(col_name='medal_id', data_type='string', comment=None),\n",
      " Row(col_name='medal_counts', data_type='int', comment=None),\n",
      " Row(col_name='ds', data_type='string', comment=None),\n",
      " Row(col_name='# Partition Information', data_type='', comment=''),\n",
      " Row(col_name='# col_name', data_type='data_type', comment='comment'),\n",
      " Row(col_name='ds', data_type='string', comment=None),\n",
      " Row(col_name='', data_type='', comment=''),\n",
      " Row(col_name='# Detailed Table Information', data_type='', comment=''),\n",
      " Row(col_name='Database', data_type='default', comment=''),\n",
      " Row(col_name='Table', data_type='daily_player_medal_counts', comment=''),\n",
      " Row(col_name='Owner', data_type='root', comment=''),\n",
      " Row(col_name='Created Time', data_type='Tue May 30 21:35:53 UTC 2023', comment=''),\n",
      " Row(col_name='Last Access', data_type='UNKNOWN', comment=''),\n",
      " Row(col_name='Created By', data_type='Spark 3.3.0', comment=''),\n",
      " Row(col_name='Type', data_type='EXTERNAL', comment=''),\n",
      " Row(col_name='Provider', data_type='PARQUET', comment=''),\n",
      " Row(col_name='Statistics', data_type='3130506 bytes, 588896 rows', comment=''),\n",
      " Row(col_name='Location', data_type='dbfs:/daily_player_medal_counts', comment=''),\n",
      " Row(col_name='Serde Library', data_type='org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe', comment=''),\n",
      " Row(col_name='InputFormat', data_type='org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat', comment=''),\n",
      " Row(col_name='OutputFormat', data_type='org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat', comment=''),\n",
      " Row(col_name='Partition Provider', data_type='Catalog', comment='')]"
     ]
    }
   ],
   "source": [
    "# Compute and update partition statistics for the table\n",
    "spark.sql(f\"ANALYZE TABLE {tableName} COMPUTE STATISTICS\")\n",
    "\n",
    "# Get the sizes of the table\n",
    "spark.sql(f\"\"\"DESC EXTENDED {tableName} \"\"\").collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Halo Data Pipeline Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
